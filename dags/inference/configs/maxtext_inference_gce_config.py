# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Utilities to construct configs for maxtext DAG."""

from xlml.apis import gcp_config, metric_config, task, test_config
from dags import test_owner
from dags.multipod.configs import common
from dags.vm_resource import TpuVersion, Project, RuntimeVersion

PROJECT_NAME = Project.CLOUD_ML_AUTO_SOLUTIONS.value
RUNTIME_IMAGE = RuntimeVersion.TPU_UBUNTU2204_BASE.value
GCS_SUBFOLDER_PREFIX = test_owner.Team.INFERENCE.value


def get_maxtext_inference_nightly_config(
    tpu_version: TpuVersion,
    tpu_cores: int,
    tpu_zone: str,
    time_out_in_min: int,
    test_name: str,
    test_mode: common.SetupMode,
    project_name: str = PROJECT_NAME,
    runtime_version: str = RUNTIME_IMAGE,
    network: str = "default",
    subnetwork: str = "default",
    is_tpu_reserved: bool = True,
    num_slices: int = 1,
) -> task.TpuQueuedResourceTask:
  job_gcp_config = gcp_config.GCPConfig(
      project_name=project_name,
      zone=tpu_zone,
      dataset_name=metric_config.DatasetOption.BENCHMARK_DATASET,
  )

  set_up_cmds = (
      "pip install --upgrade pip",

      # Download jetstream and maxtext
      "git clone -b jetstream-v0.2.0 https://github.com/google/maxtext.git",
      # "git clone -b v0.2.0 https://github.com/google/JetStream.git",
      "git clone -b consolidate_benchmark_results https://github.com/google/JetStream.git",
      # "git clone https://github.com/google/maxtext.git",
      # "git clone https://github.com/google/JetStream.git",

      # Create a python virtual environment
      "sudo apt-get -y update",
      "sudo apt-get -y install python3.10-venv",
      "python -m venv .env",
      "source .env/bin/activate",

      # Setup MaxText & JetStream
      f"cd maxtext && bash setup.sh MODE={test_mode.value} && cd ..",
      "cd JetStream/benchmarks && pip install -r requirements.in",
      "pip install torch --index-url https://download.pytorch.org/whl/cpu",
  )

  run_model_cmds = (
      # Start virtual environment
      "source .env/bin/activate",

      # Download dataset
      "wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json > /dev/null 2>&1",

      #### Checkpoint conversion
      "cd maxtext",
      # Non-Googlers please remember to point `BASE_OUTPUT_DIRECTORY` to a GCS bucket that you own, this bucket will store all the files generated by MaxText during a run
      "export BASE_OUTPUT_DIRECTORY=gs://runner-maxtext-logs",
      # Set unique id
      "idx=$(date +%Y-%m-%d-%H-%M)",
      # We define a var for the path to the Meta PyTorch checkpoint
      "export META_CHECKPOINT_PATH=gs://maxtext-llama/llama2-7b/meta-ckpt",
      # In the following command, we are copying Meta's checkpoint into a local directory `tmp`.
      "gcloud storage cp -r ${META_CHECKPOINT_PATH} /tmp/",
      # `CONVERTED_CHECKPOINT_PATH` is the path to the GCS bucket where we want to save our converted (Orbax) checkpoint. Non-Googlers please remember to point `CONVERTED_CHECKPOINT_PATH` to a GCS bucket that you own
      "export CONVERTED_CHECKPOINT_PATH=gs://maxtext-llama/test/inference/${idx}/decode-ckpt-maxtext",
      # Next, run the conversion script `MaxText/llama_or_mistral_ckpt.py` to convert Meta's PyTorch checkpoint in `base-model-path` and save the new converted (Orbax) checkpoint in the `maxtext-model-path`
      "python3 MaxText/llama_or_mistral_ckpt.py --base-model-path /tmp/meta-ckpt --model-size llama2-7b --maxtext-model-path ${CONVERTED_CHECKPOINT_PATH}",
      # We define `CONVERTED_CHECKPOINT` to refer to the checkpoint subdirectory exactly inside `CONVERTED_CHECKPOINT_PATH`. This way it is easier to use this path in the `train.py` and `decode.py` commands
      "export CONVERTED_CHECKPOINT=${CONVERTED_CHECKPOINT_PATH}/0/items",
      # Note that the `CONVERTED_CHECKPOINT` is in a `scanned` format which is great for training but for efficient decoding performance we want the checkpoint in an `unscanned` format.
      # We can do this by running `MaxText/generate_param_only_checkpoint.py` on `CONVERTED_CHECKPOINT` with `force_unroll=true`.
      "export DIRECT_PARAMETER_CHECKPOINT_RUN=direct_generate_param_only_checkpoint_${idx}",
      "python3 MaxText/generate_param_only_checkpoint.py MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} load_parameters_path=${CONVERTED_CHECKPOINT} run_name=${DIRECT_PARAMETER_CHECKPOINT_RUN} model_name='llama2-7b' force_unroll=true",
      # Like before, we define `UNSCANNED_CKPT_PATH` to refer to the checkpoint subdirectory exactly
      "export UNSCANNED_CKPT_PATH=${BASE_OUTPUT_DIRECTORY}/${DIRECT_PARAMETER_CHECKPOINT_RUN}/checkpoints/0/items",

      # We run decoding on the `UNSCANNED_CKPT_PATH` for efficient decoding on the unscanned version of the checkpoint converted directly from Meta's PyTorch checkpoint aka `CONVERTED_CHECKPOINT`. Note that this checkpoint only has parameters and no optimizer state. So, we use it by specifying`load_parameters_path=${CONVERTED_CHECKPOINT}`
      # We compare our decoded results by asserting with golden PyTorch outputs using `autoregressive_decode_assert`
      "python3 MaxText/decode.py MaxText/configs/base.yml load_parameters_path=${UNSCANNED_CKPT_PATH} run_name=runner_decode_unscanned_${idx} base_output_directory=${BASE_OUTPUT_DIRECTORY} per_device_batch_size=1 model_name='llama2-7b' ici_autoregressive_parallelism=4 max_prefill_predict_length=4  max_target_length=16 prompt=\"I love to\" autoregressive_decode_assert=\"read. I love to write. I love to share.\" attention=dot_product scan_layers=false",

      # Configure flags
      "export TOKENIZER_PATH=assets/tokenizer.llama2",
      "export LOAD_PARAMETERS_PATH=${UNSCANNED_CKPT_PATH}",
      "export MAX_PREFILL_PREDICT_LENGTH=1024",
      "export MAX_TARGET_LENGTH=2048",
      "export MODEL_NAME=llama2-7b",
      "export ICI_FSDP_PARALLELISM=1",
      "export ICI_AUTOREGRESSIVE_PARALLELISM=-1",
      "export ICI_TENSOR_PARALLELISM=1",
      "export SCAN_LAYERS=false",
      "export WEIGHT_DTYPE=bfloat16",
      "export PER_DEVICE_BATCH_SIZE=4",

      # Start JetStream MaxText server in the background
      """python MaxText/maxengine_server.py \
        MaxText/configs/base.yml \
        tokenizer_path=${TOKENIZER_PATH} \
        load_parameters_path=${LOAD_PARAMETERS_PATH} \
        max_prefill_predict_length=${MAX_PREFILL_PREDICT_LENGTH} \
        max_target_length=${MAX_TARGET_LENGTH} \
        model_name=${MODEL_NAME} \
        ici_fsdp_parallelism=${ICI_FSDP_PARALLELISM} \
        ici_autoregressive_parallelism=${ICI_AUTOREGRESSIVE_PARALLELISM} \
        ici_tensor_parallelism=${ICI_TENSOR_PARALLELISM} \
        scan_layers=${SCAN_LAYERS} \
        weight_dtype=${WEIGHT_DTYPE} \
        per_device_batch_size=${PER_DEVICE_BATCH_SIZE} > /dev/null 2>&1 &""",
      "cd ..",

      # Single test request
      "python JetStream/jetstream/tools/requester.py",

      # Run benchmark, run eval, save benchmark and eval results, and save predictions to /tmp/request-outputs.json
      f"""python JetStream/benchmarks/benchmark_serving.py \
      --tokenizer maxtext/assets/tokenizer.llama2 \
      --model llama2-7b \
      --num-prompts 1000  \
      --dataset sharegpt \
      --dataset-path ~/ShareGPT_V3_unfiltered_cleaned_split.json \
      --max-output-length 1024 \
      --request-rate 5 \
      --warmup-first true \
      --save-result \
      --save-request-outputs \
      --run-eval true""",

      "export BENCHMARK_OUTPUT=$(find . -name \"*JetStream*\" -type f -printf \"%T@ %Tc %p\n\" | sort -n | head -1 | awk 'NF>1{print $NF}')",

      # Stop JetStream server
      "kill -9 %%",

      "mv ${BENCHMARK_OUTPUT} metric_report.jsonl",
      f"gsutil cp metric_report.jsonl {metric_config.SshEnvVars.GCS_OUTPUT.value}",
  )

  job_test_config = test_config.TpuVmTest(
      test_config.Tpu(
          version=tpu_version,
          cores=tpu_cores,
          runtime_version=runtime_version,
          reserved=is_tpu_reserved,
          network=network,
          subnetwork=subnetwork,
      ),
      test_name=test_name,
      set_up_cmds=set_up_cmds,
      run_model_cmds=run_model_cmds,
      time_out_in_min=time_out_in_min,
      task_owner=test_owner.ANDY_Y,
      num_slices=num_slices,
      gcs_subfolder=f"{GCS_SUBFOLDER_PREFIX}/maxtext",
  )

  job_metric_config = metric_config.MetricConfig(
      json_lines=metric_config.JSONLinesConfig("metric_report.jsonl"),
      use_runtime_generated_gcs_folder=True,
  )

  return task.TpuQueuedResourceTask(
      task_test_config=job_test_config,
      task_gcp_config=job_gcp_config,
      task_metric_config=job_metric_config,
  )
