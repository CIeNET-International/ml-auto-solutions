{
   "accelerator": {
      "name": "v4-8",
      "numCores": 4,
      "replicas": 1,
      "size": 8,
      "type": "tpu",
      "variant": "",
      "version": 4
   },
   "command": [
      "bash",
      "llama/7B/llama2inference.sh"
   ],
   "configMaps": [
      "gcs-buckets",
      "pytorch-nfs-ip"
   ],
   "cpu": 1,
   "entrypoint": [
      "bash",
      "-cxue",
      "if [[ ! -z \"$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\" ]]; then\n  # Trim grpc:// prefix\n  export XRT_TPU_CONFIG=\"tpu_worker;0;${KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS:7}\"\nfi\n\n# Run whatever is in `command` here\ndocker-entrypoint.sh \"${@:0}\"\n"
   ],
   "frameworkPrefix": "pt-nightly",
   "image": "us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla",
   "imageTag": "nightly_3.7",
   "labels": {
      "accelerator": "v4-8",
      "benchmarkId": "pt-nightly-llama2-pjrt-infer-func-v4-8-1vm-1vm",
      "frameworkVersion": "pt-nightly",
      "mode": "func",
      "model": "llama2-pjrt-infer"
   },
   "memory": "2Gi",
   "metricConfig": {
      "sources": [
         {
            "tensorboard": {
               "aggregate_assertions": [
                  {
                     "assertion": {
                        "inclusive_bounds": false,
                        "std_devs_from_mean": {
                           "comparison": "LESS",
                           "std_devs": 5
                        },
                        "wait_for_n_data_points": 10
                     },
                     "strategy": "FINAL",
                     "tag": "ExecuteTime__Percentile_99_sec"
                  },
                  {
                     "assertion": {
                        "inclusive_bounds": true,
                        "std_devs_from_mean": {
                           "comparison": "LESS",
                           "std_devs": 0
                        },
                        "wait_for_n_data_points": 0
                     },
                     "strategy": "FINAL",
                     "tag": "aten_ops_sum"
                  }
               ],
               "exclude_tags": [
                  "LearningRate"
               ],
               "include_tags": [
                  {
                     "strategies": [
                        "FINAL"
                     ],
                     "tag_pattern": "*"
                  }
               ],
               "merge_runs": true
            }
         }
      ]
   },
   "mode": "func",
   "modelName": "llama2-pjrt-infer",
   "outputBucket": "$(OUTPUT_BUCKET)",
   "schedule": "0 7 * * *",
   "testName": "pt-nightly-llama2-pjrt-infer-func-v4-8-1vm-1vm",
   "timeout": 10800,
   "tpuSettings": {
      "preemptible": false,
      "requireTpuAvailableLabel": true,
      "reserved": "false",
      "softwareVersion": "tpu-ubuntu2204-base",
      "tpuVmCreateSleepSeconds": 90,
      "tpuVmDockerArgs": "",
      "tpuVmExports": "export PJRT_DEVICE=TPU_C_API\n",
      "tpuVmExtraSetup": "# install tokenizer model\nwget https://storage.googleapis.com/tpu-pytorch/lsiyuan-experiment/llama/spiece.model\n\n# git clone and build llama\ngit clone --branch llama2-google-next-inference https://github.com/pytorch-tpu/llama.git\ncd llama\npip3 install -r requirements.txt\npip3 install -e .\n\n# 7B config\nmkdir 7B\ncd 7B/\necho -e '{\"dim\": 4096, \"multiple_of\": 256, \"n_heads\": 32, \"n_layers\": 32, \"norm_eps\": 1e-05, \"vocab_size\": -1}' >> params.json\n\n# save llama2 test\necho -e 'python3 llama/example_text_completion.py True \"/home/xl-ml-test/llama/7B\" /home/xl-ml-test/spiece.model --max_seq_len=2048 --max_gen_len=1000 --max_batch_size=2 --dynamo=True > output.txt' >> llama2inference.sh\necho -e 'file = open(\"output.txt\")' >> getvalue.py\necho -e 'content = file.readlines()' >> getvalue.py\necho -e 'warm_line = content[-6]' >> getvalue.py\necho -e 'warm_value = float((warm_line.split())[5])' >> getvalue.py\necho -e 'if warm_value > 7.948752 or warm_value < 7.191728:' >> getvalue.py\necho -e '    raise ValueError(\"warm latency/token exceeded throuhold 7.57024 +- 5%\")' >> getvalue.py\necho -e 'else:' >> getvalue.py\necho -e '    print(\"Finished llama2 test and warm latency/token within expected throuhold 7.57024 +- 5%\")' >> getvalue.py\necho -e 'cat output.txt' >> llama2inference.sh\necho -e 'python3 llama/7B/getvalue.py' >> llama2inference.sh\ncat llama2inference.sh\n",
      "tpuVmMainCommandWorkers": "all",
      "tpuVmPytorchSetup": "pip3 install -U setuptools\n# `unattended-upgr` blocks us from installing apt dependencies\nsudo systemctl stop unattended-upgrades\nsudo apt-get -y update\nsudo apt install -y libopenblas-base\n# for huggingface tests\nsudo apt install -y libsndfile-dev\npip3 install --user --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cpu\npip install --user \\\n  'torch_xla[tpuvm] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-nightly-cp310-cp310-linux_x86_64.whl'\npip3 install pillow\ngit clone --depth=1 https://github.com/pytorch/pytorch.git\ncd pytorch\ngit clone https://github.com/pytorch/xla.git\n",
      "tpuVmStartupScript": "echo Running startup script",
      "tpuVmXlaDistPrefix": null
   },
   "volumeMap": {
      "datasets": null,
      "dshm": {
         "claim": {
            "emptyDir": {
               "medium": "Memory"
            }
         },
         "mountPath": "/dev/shm",
         "name": "dshm",
         "readOnly": false
      },
      "scripts": {
         "claim": {
            "emptyDir": {
               "medium": "Memory"
            }
         },
         "mountPath": "/scripts",
         "name": "scripts",
         "readOnly": false
      }
   }
}
