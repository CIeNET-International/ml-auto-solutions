{
   "accelerator": {
      "name": "v4-8",
      "numCores": 4,
      "replicas": 1,
      "size": 8,
      "type": "tpu",
      "variant": "",
      "version": 4
   },
   "command": [
      "bash",
      "transformers/llama2training.sh"
   ],
   "configMaps": [
      "gcs-buckets",
      "pytorch-nfs-ip"
   ],
   "cpu": 1,
   "entrypoint": [
      "bash",
      "-cxue",
      "if [[ ! -z \"$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\" ]]; then\n  # Trim grpc:// prefix\n  export XRT_TPU_CONFIG=\"tpu_worker;0;${KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS:7}\"\nfi\n\n# Run whatever is in `command` here\ndocker-entrypoint.sh \"${@:0}\"\n"
   ],
   "frameworkPrefix": "pt-2.1",
   "image": "us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla",
   "imageTag": "r2.1.0_3.8",
   "labels": {
      "accelerator": "v4-8",
      "benchmarkId": "pt-2.1-llama2-pjrt-train-spmd2b-func-v4-8-1vm-1vm",
      "frameworkVersion": "pt-2.1",
      "mode": "func",
      "model": "llama2-pjrt-train-spmd2b"
   },
   "memory": "2Gi",
   "metricConfig": {
      "sources": [
         {
            "tensorboard": {
               "aggregate_assertions": [
                  {
                     "assertion": {
                        "inclusive_bounds": false,
                        "std_devs_from_mean": {
                           "comparison": "LESS",
                           "std_devs": 5
                        },
                        "wait_for_n_data_points": 10
                     },
                     "strategy": "FINAL",
                     "tag": "ExecuteTime__Percentile_99_sec"
                  },
                  {
                     "assertion": {
                        "inclusive_bounds": true,
                        "std_devs_from_mean": {
                           "comparison": "LESS",
                           "std_devs": 0
                        },
                        "wait_for_n_data_points": 0
                     },
                     "strategy": "FINAL",
                     "tag": "aten_ops_sum"
                  }
               ],
               "exclude_tags": [
                  "LearningRate"
               ],
               "include_tags": [
                  {
                     "strategies": [
                        "FINAL"
                     ],
                     "tag_pattern": "*"
                  }
               ],
               "merge_runs": true
            }
         }
      ]
   },
   "mode": "func",
   "modelName": "llama2-pjrt-train-spmd2b",
   "outputBucket": "$(OUTPUT_BUCKET)",
   "schedule": "0 7 * * *",
   "testName": "pt-2.1-llama2-pjrt-train-spmd2b-func-v4-8-1vm-1vm",
   "timeout": 10800,
   "tpuSettings": {
      "preemptible": false,
      "requireTpuAvailableLabel": true,
      "reserved": "false",
      "softwareVersion": "tpu-ubuntu2204-base",
      "tpuVmCreateSleepSeconds": 90,
      "tpuVmDockerArgs": "",
      "tpuVmExports": "export PJRT_DEVICE=TPU_C_API\nexport XLA_USE_BF16=1\nexport XLA_IR_DEBUG=1\nexport XLA_HLO_DEBUG=1\nexport BATCH_SIZE=32\nexport NUM_EPOCH=5\nexport PROFILE_EPOCH=2\nexport PROFILE_STEP=0\nexport PROFILE_DURATION_MS=20000\nexport XLA_USE_SPMD=1\nexport PJRT_DEVICE=TPU\nexport TPU_MEGACORE=megacore_dense\n",
      "tpuVmExtraSetup": "# install tokenizer model\nwget https://storage.googleapis.com/tpu-pytorch/lsiyuan-experiment/llama/spiece.model\n\n# git clone and build transformers ### transformers/\ngit clone -b llama2-google-next-training https://github.com/pytorch-tpu/transformers.git\ncd transformers\nsudo pip3 uninstall transformers\nsudo pip3 install -e .\npip3 install datasets\npip3 install evaluate\npip3 install scikit-learn\npip3 install accelerate\npwd\nls\n\n# 2B config\nmkdir 2B\ncd 2B/\nwget https://storage.googleapis.com/manfei_public_experimental/2B.json\ncat 2B.json\n\n# save llama2 training\ncd ..\necho -e 'python3 transformers/examples/pytorch/language-modeling/run_clm.py --tokenizer_name gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 32 --per_device_eval_batch_size 8 --num_train_epochs 1 --do_train --output_dir /tmp/output --overwrite_output_dir --config_name transformers/2B/2B.json --save_strategy no --logging_strategy no --remove_unused_columns no --spmd_fsdp_sharding --torch_dtype bfloat16 --dataloader_drop_last yes --spmd_grad_chkpt --report_to none > output.txt' >> llama2training.sh\necho -e 'import numpy as np' >> getvalue.py\necho -e 'file = open(\"output.txt\")' >> getvalue.py\necho -e 'content = file.readlines()' >> getvalue.py\necho -e 'value_line = content[-1]' >> getvalue.py\necho -e 'value_value = float((value_line.split())[2])' >> getvalue.py\necho -e 'value_value = np.reciprocal(value_value)' >> getvalue.py\necho -e 'if value_value > 6.863 or value_value < 6.209 :' >> getvalue.py\necho -e '    raise ValueError(\"expose to train_steps_per_second exceeded throuhold 6.536 +- 5%\")' >> getvalue.py\necho -e 'else:' >> getvalue.py\necho -e '    print(\"Finished llama2 test and warm latency/token within expected throuhold 6.536 +- 5%\")' >> getvalue.py\necho -e 'cat output.txt' >> llama2training.sh\necho -e 'python3 transformers/getvalue.py' >> llama2training.sh\ncat llama2training.sh\npwd\nls\n",
      "tpuVmMainCommandWorkers": "all",
      "tpuVmPytorchSetup": "pip3 install -U setuptools\n# `unattended-upgr` blocks us from installing apt dependencies\nsudo systemctl stop unattended-upgrades\nsudo apt-get -y update\nsudo apt install -y libopenblas-base\n# for huggingface tests\nsudo apt install -y libsndfile-dev\n# TODO change back to torch2.1 once pytorch released torch2.1\npip install --user --pre --no-deps torch torchvision --extra-index-url https://download.pytorch.org/whl/test/cpu\npip install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.1.0-cp310-cp310-manylinux_2_28_x86_64.whl\npip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\npip3 install pillow\npip3 install typing_extensions\npip3 install sympy\ngit clone --depth=1 -b release/2.1 https://github.com/pytorch/pytorch.git\ncd pytorch\ngit clone -b r2.1 https://github.com/pytorch/xla.git\n",
      "tpuVmStartupScript": "echo Running startup script",
      "tpuVmXlaDistPrefix": null
   },
   "volumeMap": {
      "datasets": null,
      "dshm": {
         "claim": {
            "emptyDir": {
               "medium": "Memory"
            }
         },
         "mountPath": "/dev/shm",
         "name": "dshm",
         "readOnly": false
      },
      "scripts": {
         "claim": {
            "emptyDir": {
               "medium": "Memory"
            }
         },
         "mountPath": "/scripts",
         "name": "scripts",
         "readOnly": false
      }
   }
}
